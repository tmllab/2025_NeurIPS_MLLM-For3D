# "nuscenes" or "kitti"
dataset: scannetpp
mode: pretrain
#mode: 'source_free'
working_dir: output/reasonseg/scannetpp/
# if set to True, use cylindrical coordinates, otherwise use cartesian
cylindrical_coordinates: false
# size of the voxel in each dimension for cartesian coordinates,
# and in rho and z for cylindrical (angular is always 1Â°)
voxel_size: 0.05
batch_size: 1
# learning rate
lr: 0.01
sgd_momentum: 0.9
sgd_dampening: 0.1
weight_decay: 0.0001
num_epochs: 40
# used in superpixel loss only, drop points and pixels from the computation of the loss
dropout: 0.
# number of GPUs and CPU threads to use
num_gpus: 1
num_threads: 32
kernel_size: 3
model_n_out: 512
bn_momentum: 0.05
crop_size:
  - 224
  - 416
crop_ratio:
  - 1.5555555555555556
  - 1.8888888888888888
# crop_size:
#   - 448
#   - 832
# point cloud backbone to use among "minkunet" and "voxelnet"
model_points: minkunet
# which image pretraining to chose among:
# 'imagenet','obow', 'pixpro', 'moco_v1', 'moco_v2', 'swav',
# 'deepcluster_v2', 'dino', 'moco_coco'
image_weights: moco_v2
#image_weights : None
# which image encoder to use (only imagenet is available with resnet18)
#images_encoder : "resnet50"
images_encoder: lisa
# which image decoder to use
# 'bilinear', 'unet', 'fpn', 'semseg', 'nnfe', 'dilation', 'ppkt'
decoder: dilation
# temperature parameter in the InfoNCE loss
NCE_temperature: 0.07
# number of positive matches in the InfoNCE loss
num_matches: 4096
# whether to use the true validation set or the custom parametrization set
training: training
# transformations to apply to the clouds
#transforms_images : ["RandomContrast", "RandomBrightness", "RandomSaturation", "RandomLighting"]
#transforms_images : ["ColorAugSSDTransform"]
transforms_images: []
transforms_clouds:
  - Rotation
  - FlipAxis
# transformations to apply to both the clouds and the images among:
transforms_mixed:
  - ResizedCrop
  - FlipHorizontal
text_augmentation: true
# which losses to use (note that multiple losses will be summed)
losses:
  - scannet_loss
# LISA inference
version: xinlai/LISA-7B-v1
precision: bf16
conv_type: llava_v1
vision_tower: openai/clip-vit-large-patch14
model_max_length: 512
local_rank: 0
image_size: 1024
use_mm_start_end: True
vis_save_path: ./vis_results

points_path: /workspaces/liziwen/hjxmbzuai/datasets/scannetpp/prepare_sem_data
dataRoot_scannetpp: /l/users/jiaxin.huang/jiaxin/dataset/scannetpp
# train_file: /workspaces/liziwen/hjxmbzuai/datasets/scannetpp/splits/train_2.txt
# val_file: /workspaces/liziwen/hjxmbzuai/datasets/scannetpp/splits/val_2.txt
train_file: /l/users/jiaxin.huang/jiaxin/dataset/scannetpp/splits/train_test.txt
val_file: /l/users/jiaxin.huang/jiaxin/dataset/scannetpp/splits/train_test.txt
train_description: /l/users/jiaxin.huang/jiaxin/instruct3D_val_new.json
val_description: /l/users/jiaxin.huang/jiaxin/instruct3D_val_new.json
# which kind of superpixels to use
superpixels_type: slic
# only keep 1 in dataset_skip_step training examples (here use 100% of the data)
dataset_skip_step: 1
# path to weights to continue a previous training
resume_path: null

max_sweeps: 1

# text_embeddings_all_path: /home/jiaxin.huang/jiaxin/CLIP2Scene/scannetpp_top100_ViT16_all_clip_text.pth
# text_categories: 100
# text_embeddings_path: '/home/jiaxin.huang/jiaxin/CLIP2Scene/scannetpp_top100_ViT16_clip_text.pth'

# maskclip_checkpoint: /home/jiaxin.huang/jiaxin/CLIP2Scene/ViT16_clip_backbone.pth
# visual_projs_path: /home/jiaxin.huang/jiaxin/CLIP2Scene/ViT16_clip_weights.pth

# WARNING: DO NOT CHANGE THE FOLLOWING PARAMETERS
# ===============================================
normalize_features: false
superpixel_size: 150